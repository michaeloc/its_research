{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding features to Trajectories\n",
    "\n",
    "Features used here:\n",
    "\n",
    "['lat','lng','instante','rota','velocidade','posicao','viaje','matricula_id','lat_uber','lng_uber','label']\n",
    "\n",
    "Adding features:\n",
    "\n",
    "* Acceleration\n",
    "* Bearing\n",
    "* Weekdays\n",
    "* Trajectory id\n",
    "* Point id\n",
    "* Statistic features\n",
    "* Flag (is_noise ? 0 - Not noise, 1 - Noise)\n",
    "\n",
    "\n",
    "**The mainly goals here is to generate data to feed the PAC method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import sys  \n",
    "from sentences import Sentences\n",
    "sys.path.insert(0, '/home/mobility/michael/segmentation/its_research')\n",
    "from labeling.labels import Labels\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GTFS_STOP_PATH = '../dublin/dublin/bus_stops.csv'\n",
    "SEMAFORO_PATH ='../dublin/dublin/traffic_signals.csv'\n",
    "DATA = '../data/sentences_dublin_labeled.npy'\n",
    "\n",
    "stops = pd.read_csv(GTFS_STOP_PATH)\n",
    "trfl = pd.read_csv(SEMAFORO_PATH)\n",
    "dt = np.load(DATA,allow_pickle=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding other_stop label\n",
    "\n",
    "Here, we also encoding the labels to :\n",
    "* 0 - bus_stop\n",
    "* 1 - in_route\n",
    "* 2 - other_stop\n",
    "* 3 - traffic signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['lat','lng','instante','rota','velocidade','posicao','viaje','matricula_id','lat_uber','lng_uber','label']\n",
    "sentences = Sentences(features)\n",
    "labels = Labels(20,30,stops,trfl)\n",
    "labels.add_other_stop_label(dt)\n",
    "sentences.label_encoder(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick analisys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Verifying the number of points by trajectories\n",
    "average_size_trajectory = list()\n",
    "for items in dt:\n",
    "    average_size_trajectory.append(len(items))\n",
    "print(f'mean:{np.mean(average_size_trajectory)}\\t std:{np.std(average_size_trajectory)}\\t min:{np.min(average_size_trajectory)}\\tmax:{np.max(average_size_trajectory)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Verifying average distance by trajectories\n",
    "average_size_distance = list()\n",
    "for items in dt:\n",
    "    average_size_distance.append(items[-1][5])\n",
    "print(f'mean:{np.mean(average_size_distance)}\\t std:{np.std(average_size_distance)}\\t min:{np.min(average_size_distance)}\\tmax:{np.max(average_size_distance)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtd_routes = {x[0][3]:0 for x in dt}\n",
    "for items in dt:\n",
    "    qtd_routes[items[0][3]]+=1\n",
    "print(f'qnt of distinct routes: {len(list(qtd_routes.keys()))}')\n",
    "print(f'routes with grater num of traj: {max(qtd_routes, key=lambda k: qtd_routes[k])}')\n",
    "print(f'routes with lesser num of traj: {min(qtd_routes, key=lambda k: qtd_routes[k])}')\n",
    "print(f'mean: {np.mean(list(qtd_routes.values()))}')\n",
    "print(f'std: {np.std(list(qtd_routes.values()))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences.add_features(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('dt_features.npy',dt)\n",
    "# dt = np.load('dt_features.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = list(map(sentences.complete_trajectory, dt, [96]*len(dt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing false labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing all other_stop ex: ex: '''bus_stop > other_stop > bus_stop'''\n",
    "labels.get_false_labels(dt, 2.0, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting only important features\n",
    "\n",
    "Here, we also add trajectory id, point id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17701it [00:22, 788.34it/s]\n"
     ]
    }
   ],
   "source": [
    "selected_dt = sentences.select_features(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding noise id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17701it [00:01, 15445.84it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences.add_id_noise(selected_dt, set([]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning time into milliseconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17701/17701 [01:46<00:00, 166.62it/s]\n"
     ]
    }
   ],
   "source": [
    "dt_with_new_time = sentences.get_time_in_seconds(selected_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting pad\n",
    "\n",
    "### Optional: Adding pad 0 to begining of trajectory and repeat the last points\n",
    "\n",
    "Without padding, statistic function does not take into account the initial points\n",
    "\n",
    "\n",
    "It is important if we need to generate embeddings to model STOD, because we need all trajectory points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_with_padding = sentences.padding(16, np.array(dt_with_new_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding statistics features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17701/17701 [42:30<00:00,  6.94it/s]  \n"
     ]
    }
   ],
   "source": [
    "final_data = sentences.put_statistics_metrics_with_padding(np.array(dt_with_padding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_h5py(final_list_0,final_list_1,final_list_2,final_list_3,final_list_4,final_list_5, final_list_6):\n",
    "    with h5py.File('../data/pac_data/data_to_pac.hdf5', \"w\") as  hdf5_store:\n",
    "        list_stat_x_b = hdf5_store.create_dataset(\"list_window_x_b\",data=final_list_0, compression=\"gzip\")\n",
    "        list_stat_x_a = hdf5_store.create_dataset(\"list_window_x_a\",data=final_list_1, compression=\"gzip\")\n",
    "        list_stat_x_c = hdf5_store.create_dataset(\"list_x_c_queries\",data=final_list_2, compression=\"gzip\")\n",
    "        list_stat_x_bs = hdf5_store.create_dataset(\"list_window_x_before_stats\",data=final_list_3, compression=\"gzip\")\n",
    "        list_stat_x_as = hdf5_store.create_dataset(\"list_window_x_after_stats\",data=final_list_4, compression=\"gzip\")\n",
    "        list_stat_y = hdf5_store.create_dataset(\"list_y_queries\",data=final_list_5, compression=\"gzip\")\n",
    "        list_stat_y_list = hdf5_store.create_dataset(\"list_window_y\",data=final_list_6, compression=\"gzip\")\n",
    "        hdf5_store.flush()\n",
    "        hdf5_store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_h5py(sentences_to_feed_model[0],\n",
    "          sentences_to_feed_model[1],\n",
    "          sentences_to_feed_model[2],\n",
    "          sentences_to_feed_model[3],\n",
    "          sentences_to_feed_model[4],\n",
    "          sentences_to_feed_model[5],\n",
    "          sentences_to_feed_model[6])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
